<!DOCTYPE html>
<html>

<head>
  <script defer src="https://cloud.umami.is/script.js" data-website-id="028ba410-097e-443c-a63b-7e6f79597d30"></script>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="VisualSphinx">
  <meta property="og:title" content="VisualSphinx" />
  <meta property="og:description" content="Large-Scale Synthetic Vision Logic Puzzles for RL" />
  <meta property="og:url" content="https://visualsphinx.github.io/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="VisualSphinx">
  <meta name="twitter:description" content="Large-Scale Synthetic Vision Logic Puzzles for RL">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Large-Scale Synthetic Vision Logic Puzzles for RL">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VisualSphinx</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL
              for Coding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="" target="_blank">Yichen Feng</a><sup>♠</sup>,</span>
              <span class="author-block">
                <a href="https://zhangchenxu.com/" target="_blank">Zhangchen Xu</a><sup>♠</sup>,</span>
              <span class="author-block">
                <a href="https://fqjiang.work/" target="_blank">Fengqing Jiang</a><sup>♠</sup>,</span>
              <span class="author-block">
                <a href="https://yuetl9.github.io/" target="_blank">Yuetai Li</a><sup>♠</sup>,</span>
              <br><span class="author-block">
                <a href="https://sites.google.com/view/rbhaskar" target="_blank">Bhaskar Ramasubramanian</a><sup>♢</sup>,</span>
              <span class="author-block">
                <a href="https://luyaoniu.github.io/" target="_blank">Luyao Liu</a><sup>♠</sup>,</span>
              <span class="author-block">
                <a href="https://yuchenlin.xyz/" target="_blank">Bill Yuchen Lin</a><sup>♠</sup>,</span>
              <span class="author-block">
                <a href="https://people.ece.uw.edu/radha/" target="_blank">Radha Poovendran</a><sup>♠</sup>,</span>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>♠</sup>University of Washington, <sup>♢</sup>Western Washington University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- HuggingFace Organization link -->
                <span class="link-block">
                  <a href="https://huggingface.co/VisualSphinx" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      🤗
                    </span>
                    <span>HF Org</span>
                  </a>
                </span>

                <!-- HuggingFace Dataset link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/VisualSphinx/VisualSphinx-V1-RL-20K" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>VisualSphinx-V1 (RL)</span>
                  </a>
                </span>

                <!-- HuggingFace Dataset link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/VisualSphinx/VisualSphinx-V1-Benchmark" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>VisualSphinx-Test (Benchmark)</span>
                  </a>
                </span>


                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/VisualSphinx/VisualSphinx" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Codebase</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent
              decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM
              reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose <b>VisualSphinx</b>, a
              first-of-its-kind <b>large-scale</b> synthetic visual logical reasoning training data. To tackle the challenge of image
              synthesis with grounding answers, we propose a <b>rule-to-image synthesis pipeline</b>, which extracts and expands puzzle rules
              from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly.
              Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of
              our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed
              from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry
              reasoning.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Optional section title -->
      <div class="has-text-centered">
        <h2 class="title is-3">VisualSphinx Examples</h2>
      </div>
      <br>
      <!-- Figure on top -->

      <figure class="image" style="width: 95%; margin: 0 auto;">
        <img src="static/images/examples.jpg" alt="" />
      </figure>
      <br>
      <p class="has-text-left">
        This figure demonstrates the <b>VisualSphinx-V1</b> instances within each reasoning category. Each visual logic puzzle comprises a text prompt, a
        graphical question stem with four images and a question mark, and four candidate choices of graphical answers.
      </p>
    </div>
  </section>

  <section class="section"></section>
  <div class="container is-max-desktop">
    <!-- Optional section title -->
    <div class="has-text-centered">
      <h2 class="title is-3">✨ VisualSphinx Pipeline ✨</h2>
    </div>
    <br>
    <!-- Figure on top -->

    <figure class="image" style="width: 95%; margin: 0 auto;">
      <img src="static/images/pipeline.jpg" alt="" />
    </figure>
    <br>
    <p class="has-text-left">
      This figure illustrates the <b>four-stage</b> pipeline for generating <b>VisualSphinx-V1</b>. In Step 1, we collect 4K seed
      puzzles with explanations and abstract them into structured rule descriptions using LLMs. In Step 2, we apply a
      rule-level genetic algorithm to cross over, mutate and diversify the seed rules, scaling them to 40K high-quality rules.
      In Step 3, each rule is paired with a rendering style and used to generate five correct and three incorrect images via
      LLM-generated Python scripts. The fifth correct image is designated as the answer option, while the three rule-breaking
      images serve as distractors. After deduplication, we obtain 110K image groups. In Step 4, we assemble puzzles from each
      group using three complementary strategies: default assembly, shuffled answer variants, and expanded distractor sets.
      This results in over 660K visual logic puzzles, enabling robust and diverse training for multimodal reasoning models.
    </p>
  </div>
  </section>

  <section class="section"></section>
  <div class="container is-max-desktop">
    <!-- Optional section title -->
    <div class="has-text-centered">
      <h2 class="title is-3">✨ VisualSphinx is High-quality! ✨</h2>
    </div>
    <br>
    <!-- Figure on top -->

    <figure class="image" style="width: 95%; margin: 0 auto;">
      <img src="static/images/statistics.jpg" alt="" />
    </figure>
    <br>
    <p class="has-text-left">
      This figure shows the statistics of readability, logical coherence and pass rates of <b>VisualSphinx-V1</b>.
    </p>
  </div>
  </section>

  <section class="section"></section>
  <div class="container is-max-desktop">
    <!-- Optional section title -->
    <div class="has-text-centered">
      <h2 class="title is-3">✨ Performance on Our Benchmarks ✨</h2>
    </div>
    <br>
    <!-- Figure on top -->

    <figure class="image" style="width: 95%; margin: 0 auto;">
      <img src="static/images/benchmark.png" alt="" />
    </figure>
    <br>
    <p class="has-text-left">
      This figure illustrates the performance of the model trained on <b>VisualSphinx-V1</b>, evaluated on the <b>VisualSphinx-TEST</b> across varying difficulty levels, and compares it with other models.
    </p>
    <br>
  </div>
  </section>

  <!-- Image carousel -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
  <!-- <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
  <!-- <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
  <!-- <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
  <!-- <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Fourth image description.
        </h2>
    </div>
  </div>
</div>
</div>
</section> -->
  <!-- End image carousel -->



  <!--
  Kodkod introduction and video
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">🐱 Meet Kodkod: Nature's Stealthy Programmer! 🐾</h2>
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified">
              <p>
                Kodkod (Leopardus guigna) is a wild cat species native to the Americas, known for its adaptability and
                resilience in various environments. Just as the Kodkod navigates through diverse habitats, from dense
                forests to open areas, KodCode is designed to handle various programming challenges with agility and
                precision.
              </p>
            </div>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/BFA6hxfnVjM" frameborder="0" allow="autoplay; encrypted-media"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  End Kodkod introduction and video
  -->


  <!-- 
Video carousel
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            Your video file here
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            Your video file here
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            Your video file here
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
End video carousel 
-->


  <!-- 
Paper poster
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
End paper poster 
-->

  <!-- 
Teaser video
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Why we name it KodCode?
      </h2>
    </div>
  </div>
</section>
End teaser video 
-->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{feng2025visualsphinx,
        title={VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL}, 
        author={Yichen Feng and Zhangchen Xu and Fengqing Jiang and Yuetai Li and Bhaskar Ramasubramanian and Luyao Niu and Bill Yuchen Lin and Radha Poovendran},
        year={2025},
        eprint={},
        archivePrefix={arXiv},
        primaryClass={},
        url={}, 
  }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>